{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f84831-b266-4e86-9444-ac374966302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bbbc56-3453-46d7-ba25-aed7e9bc670a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b9cff-50a7-4166-b490-07a28c458d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 14 01:31:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.13                 Driver Version: 556.13         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P0              9W /   50W |    3034MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8580      C   ...niconda3\\envs\\pytorchenv\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     18568      C   ...niconda3\\envs\\pytorchenv\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     18572      C   ...niconda3\\envs\\pytorchenv\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Function to monitor GPU usage in a loop\n",
    "def monitor_gpu_usage():\n",
    "    try:\n",
    "        while True:\n",
    "            # Call nvidia-smi command to get the GPU stats\n",
    "            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            \n",
    "            # Clear the output of the previous execution to update it\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Print the GPU status\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Wait for a short period before checking again (e.g., every 1 second)\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"GPU monitoring stopped.\")\n",
    "\n",
    "# Run the GPU monitor in the notebook\n",
    "monitor_gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2862a1-bdee-4345-8449-cdf86257506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.version.cuda)  # Check CUDA version PyTorch is compiled with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be9793d-402d-440c-8f3f-5185ace4ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "print(torch.version.cuda)         # Should now match the installed CUDA version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec904d4-fd2a-4cf1-b49a-6da341e8e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 2050\n",
      "Checking GPU usage...\n",
      "Tensor operation completed on GPU.\n",
      "Memory allocated on GPU: 764.00 MB\n",
      "Memory cached by PyTorch: 764.00 MB\n",
      "Current GPU utilization: 0%\n",
      "CUDA version: 12.4\n",
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.cuda\n",
    "\n",
    "def check_gpu_usage():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "        return\n",
    "\n",
    "    # Check if the GPU is being utilized by PyTorch\n",
    "    print(\"Checking GPU usage...\")\n",
    "    # Run a simple tensor operation to ensure GPU is in use\n",
    "    dummy_tensor = torch.rand((10000, 10000), device=device)\n",
    "    result = dummy_tensor * dummy_tensor\n",
    "    print(f\"Tensor operation completed on GPU.\")\n",
    "\n",
    "    # Print GPU memory and usage\n",
    "    print(f\"Memory allocated on GPU: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n",
    "    print(f\"Memory cached by PyTorch: {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n",
    "    print(f\"Current GPU utilization: {torch.cuda.utilization(device)}%\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Run the check\n",
    "check_gpu_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff1540-5a57-4e81-88e7-ae57259e8f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763614c-fb83-4ef4-8833-51cdd5762601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2538e-b007-4d3e-b1dc-f08fe1084f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
